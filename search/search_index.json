{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Dython \u00b6 Welcome! \u00b6 Dython is a set of D ata analysis tools in p YTHON 3.x, which can let you get more insights about your data. Dython was designed with analysis usage in mind - meaning ease-of-use, functionality and readability are the core values of this library. Production-grade performance, on the other hand, were not considered. Here are some cool things you can do with it: Given a dataset, Dython will automatically find which features are categorical and which are numerical, compute a relevant measure of association between each and every feature, and plot it all as an easy-to-read heat-map. And all this is done with a single line: from dython.nominal import associations associations ( data ) The result: Here's another thing - given a machine-learning multi-class model's predictions, you can easily display each class' ROC curve, AUC score and find the estimated-optimal thresholds - again, with a single line of code: from dython.model_utils import roc_graph roc_graph ( y_true , y_pred ) The result: Installation \u00b6 Dython can be installed directly using pip : pip install dython Other installation options are available, see the installation page for more information. Examples \u00b6 See some usage examples of nominal.associations and model_utils.roc_graph on the examples page . All examples can also be imported and executed from dython.examples . Modules Documentation \u00b6 Full documentation of all modues and public methods is available: nominal model_utils sampling Related blogposts \u00b6 Read more about the dython.nominal tools on The Search for Categorical Correlation Read more about using ROC graphs on Hard ROC: Really Understanding & Properly Using ROC and AUC","title":"Home"},{"location":"#dython","text":"","title":"Dython"},{"location":"#welcome","text":"Dython is a set of D ata analysis tools in p YTHON 3.x, which can let you get more insights about your data. Dython was designed with analysis usage in mind - meaning ease-of-use, functionality and readability are the core values of this library. Production-grade performance, on the other hand, were not considered. Here are some cool things you can do with it: Given a dataset, Dython will automatically find which features are categorical and which are numerical, compute a relevant measure of association between each and every feature, and plot it all as an easy-to-read heat-map. And all this is done with a single line: from dython.nominal import associations associations ( data ) The result: Here's another thing - given a machine-learning multi-class model's predictions, you can easily display each class' ROC curve, AUC score and find the estimated-optimal thresholds - again, with a single line of code: from dython.model_utils import roc_graph roc_graph ( y_true , y_pred ) The result:","title":"Welcome!"},{"location":"#installation","text":"Dython can be installed directly using pip : pip install dython Other installation options are available, see the installation page for more information.","title":"Installation"},{"location":"#examples","text":"See some usage examples of nominal.associations and model_utils.roc_graph on the examples page . All examples can also be imported and executed from dython.examples .","title":"Examples"},{"location":"#modules-documentation","text":"Full documentation of all modues and public methods is available: nominal model_utils sampling","title":"Modules Documentation"},{"location":"#related-blogposts","text":"Read more about the dython.nominal tools on The Search for Categorical Correlation Read more about using ROC graphs on Hard ROC: Really Understanding & Properly Using ROC and AUC","title":"Related blogposts"},{"location":"getting_started/examples/","text":"Examples \u00b6 Examples can be imported and executed from dython.examples . associations_iris_example() \u00b6 Plot an example of an associations heat-map of the Iris dataset features. All features of this dataset are numerical (except for the target). Example code: import pandas as pd from sklearn import datasets from dython.nominal import associations # Load data iris = datasets . load_iris () # Convert int classes to strings to allow associations # method to automatically recognize categorical columns target = [ 'C {} ' . format ( i ) for i in iris . target ] # Prepare data X = pd . DataFrame ( data = iris . data , columns = iris . feature_names ) y = pd . DataFrame ( data = target , columns = [ 'target' ]) df = pd . concat ([ X , y ], axis = 1 ) # Plot features associations associations ( df ) Output: associations_mushrooms_example() \u00b6 Plot an example of an associations heat-map of the UCI Mushrooms dataset features. All features of this dataset are categorical. This example will use Theil's U. Example code: import pandas as pd from dython.nominal import associations # Download and load data from UCI df = pd . read_csv ( 'http://archive.ics.uci.edu/ml/machine-learning-databases/mushroom/agaricus-lepiota.data' ) df . columns = [ 'class' , 'cap-shape' , 'cap-surface' , 'cap-color' , 'bruises' , 'odor' , 'gill-attachment' , 'gill-spacing' , 'gill-size' , 'gill-color' , 'stalk-shape' , 'stalk-root' , 'stalk-surface-above-ring' , 'stalk-surface-below-ring' , 'stalk-color-above-ring' , 'stalk-color-below-ring' , 'veil-type' , 'veil-color' , 'ring-number' , 'ring-type' , 'spore-print-color' , 'population' , 'habitat' ] # Plot features associations associations ( df , theil_u = True , figsize = ( 15 , 15 )) Output: roc_graph_example() \u00b6 Plot an example ROC graph of an SVM model predictions over the Iris dataset. Based on sklearn examples (as was seen on April 2018). Example code: import numpy as np from sklearn import svm , datasets from sklearn.model_selection import train_test_split from sklearn.preprocessing import label_binarize from sklearn.multiclass import OneVsRestClassifier from dython.model_utils import roc_graph # Load data iris = datasets . load_iris () X = iris . data y = label_binarize ( iris . target , classes = [ 0 , 1 , 2 ]) # Add noisy features random_state = np . random . RandomState ( 4 ) n_samples , n_features = X . shape X = np . c_ [ X , random_state . randn ( n_samples , 200 * n_features )] # Train a model X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size =. 5 , random_state = 0 ) classifier = OneVsRestClassifier ( svm . SVC ( kernel = 'linear' , probability = True , random_state = 0 )) # Predict y_score = classifier . fit ( X_train , y_train ) . predict_proba ( X_test ) # Plot ROC graphs roc_graph ( y_test , y_score , class_names = iris . target_names ) Output: Note: Due to the nature of np.random.RandomState which is used in this example, the output graph may vary from one machine to another.","title":"Examples"},{"location":"getting_started/examples/#examples","text":"Examples can be imported and executed from dython.examples .","title":"Examples"},{"location":"getting_started/examples/#associations_iris_example","text":"Plot an example of an associations heat-map of the Iris dataset features. All features of this dataset are numerical (except for the target). Example code: import pandas as pd from sklearn import datasets from dython.nominal import associations # Load data iris = datasets . load_iris () # Convert int classes to strings to allow associations # method to automatically recognize categorical columns target = [ 'C {} ' . format ( i ) for i in iris . target ] # Prepare data X = pd . DataFrame ( data = iris . data , columns = iris . feature_names ) y = pd . DataFrame ( data = target , columns = [ 'target' ]) df = pd . concat ([ X , y ], axis = 1 ) # Plot features associations associations ( df ) Output:","title":"associations_iris_example()"},{"location":"getting_started/examples/#associations_mushrooms_example","text":"Plot an example of an associations heat-map of the UCI Mushrooms dataset features. All features of this dataset are categorical. This example will use Theil's U. Example code: import pandas as pd from dython.nominal import associations # Download and load data from UCI df = pd . read_csv ( 'http://archive.ics.uci.edu/ml/machine-learning-databases/mushroom/agaricus-lepiota.data' ) df . columns = [ 'class' , 'cap-shape' , 'cap-surface' , 'cap-color' , 'bruises' , 'odor' , 'gill-attachment' , 'gill-spacing' , 'gill-size' , 'gill-color' , 'stalk-shape' , 'stalk-root' , 'stalk-surface-above-ring' , 'stalk-surface-below-ring' , 'stalk-color-above-ring' , 'stalk-color-below-ring' , 'veil-type' , 'veil-color' , 'ring-number' , 'ring-type' , 'spore-print-color' , 'population' , 'habitat' ] # Plot features associations associations ( df , theil_u = True , figsize = ( 15 , 15 )) Output:","title":"associations_mushrooms_example()"},{"location":"getting_started/examples/#roc_graph_example","text":"Plot an example ROC graph of an SVM model predictions over the Iris dataset. Based on sklearn examples (as was seen on April 2018). Example code: import numpy as np from sklearn import svm , datasets from sklearn.model_selection import train_test_split from sklearn.preprocessing import label_binarize from sklearn.multiclass import OneVsRestClassifier from dython.model_utils import roc_graph # Load data iris = datasets . load_iris () X = iris . data y = label_binarize ( iris . target , classes = [ 0 , 1 , 2 ]) # Add noisy features random_state = np . random . RandomState ( 4 ) n_samples , n_features = X . shape X = np . c_ [ X , random_state . randn ( n_samples , 200 * n_features )] # Train a model X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size =. 5 , random_state = 0 ) classifier = OneVsRestClassifier ( svm . SVC ( kernel = 'linear' , probability = True , random_state = 0 )) # Predict y_score = classifier . fit ( X_train , y_train ) . predict_proba ( X_test ) # Plot ROC graphs roc_graph ( y_test , y_score , class_names = iris . target_names ) Output: Note: Due to the nature of np.random.RandomState which is used in this example, the output graph may vary from one machine to another.","title":"roc_graph_example()"},{"location":"getting_started/installation/","text":"Installing Dython \u00b6 Installation \u00b6 The easiest way to install dython is using pip install : pip install dython If you'd like to use the source code instead, you can install directly from it using any of the following methods: Install source code using pip: pip install git+https://github.com/shakedzy/dython.git ` Download the source code as a ZIP file Download the source code as a TAR ball Dependencies \u00b6 Dython requires Python 3.5 or higher, and all of th following packages: numpy pandas seaborn scipy matplotlib sklearn","title":"Installation"},{"location":"getting_started/installation/#installing-dython","text":"","title":"Installing Dython"},{"location":"getting_started/installation/#installation","text":"The easiest way to install dython is using pip install : pip install dython If you'd like to use the source code instead, you can install directly from it using any of the following methods: Install source code using pip: pip install git+https://github.com/shakedzy/dython.git ` Download the source code as a ZIP file Download the source code as a TAR ball","title":"Installation"},{"location":"getting_started/installation/#dependencies","text":"Dython requires Python 3.5 or higher, and all of th following packages: numpy pandas seaborn scipy matplotlib sklearn","title":"Dependencies"},{"location":"modules/model_utils/","text":"model_utils \u00b6 roc_graph \u00b6 roc_graph(y_true, y_pred, micro=True, macro=True, eoptimal_threshold=True, class_names=None, colors=None, ax=None, figsize=None, xlim=(0.,1.), ylim=(0.,1.02), lw=2, ls='-',ms=10,fmt='.2f') Plot a ROC graph of predictor's results (inclusding AUC scores), where each row of y_true and y_pred represent a single example. If there are 1 or two columns only, the data is treated as a binary classification (see input example below). If there are more then 2 columns, each column is considered a unique class, and a ROC graph and AUC score will be computed for each. A Macro-ROC and Micro-ROC are computed and plotted too by default. Based on scikit-learn examples (as was seen on April 2018): y_true : list / NumPy ndarray The true classes of the predicted data y_pred : list / NumPy ndarray The predicted classes micro : Boolean Default = True Whether to calculate a Micro ROC graph (not applicable for binary cases) macro : Boolean Default = True Whether to calculate a Macro ROC graph (not applicable for binary cases) eoptimal_threshold : Boolean Default = True Whether to calculate and display the estimated-optimal threshold for each ROC graph. The estimated-optimal threshold is the closest computed threshold with (fpr,tpr) values closest to (0,1) class_names : list or string Default = None Names of the different classes. In a multi-class classification, the order must match the order of the classes probabilities in the input data. In a binary classification, can be a string or a list. If a list, only the last element will be used. colors : list of Matplotlib color strings or None Default = None List of colors to be used for the plotted curves. If None , falls back to a predefined default. ax : matplotlib ax Default = None Matplotlib Axis on which the curves will be plotted figsize : (int,int) or None Default = None A Matplotlib figure-size tuple. If None , falls back to Matplotlib's default. Only used if ax=None . xlim : (float, float) Default = (0.,1.) X-axis limits. ylim : (float,float) Default = (0.,1.02) Y-axis limits. lw : int Default = 2 Line-width. ls : string Default = '-' Matplotlib line-style string ms : int Default = 10 Marker-size. fmt : string Default = '.2f' String formatting of displayed AUC and threshold numbers. Returns: A dictionary, one key for each class. Each value is another dictionary, holding AUC and eOpT values. Example: See examples . Binary Classification Input Example: Consider a data-set of two data-points where the true class of the first line is class 0, which was predicted with a probability of 0.6, and the second line's true class is 1, with predicted probability of 0.8. # First option: >> roc_graph ( y_true = [ 0 , 1 ], y_pred = [ 0.6 , 0.8 ]) # Second option: >> roc_graph ( y_true = [[ 1 , 0 ],[ 0 , 1 ]], y_pred = [[ 0.6 , 0.4 ],[ 0.2 , 0.8 ]]) # Both yield the same result random_forest_feature_importance \u00b6 random_forest_feature_importance(forest, features, precision=4) Given a trained sklearn.ensemble.RandomForestClassifier , plot the different features based on their importance according to the classifier, from the most important to the least. forest : sklearn.ensemble.RandomForestClassifier A trained RandomForestClassifier features : list A list of the names of the features the classifier was trained on, ordered by the same order the appeared in the training data precision : int Default = 4 Precision of feature importance.","title":"model_utils"},{"location":"modules/model_utils/#model_utils","text":"","title":"model_utils"},{"location":"modules/model_utils/#roc_graph","text":"roc_graph(y_true, y_pred, micro=True, macro=True, eoptimal_threshold=True, class_names=None, colors=None, ax=None, figsize=None, xlim=(0.,1.), ylim=(0.,1.02), lw=2, ls='-',ms=10,fmt='.2f') Plot a ROC graph of predictor's results (inclusding AUC scores), where each row of y_true and y_pred represent a single example. If there are 1 or two columns only, the data is treated as a binary classification (see input example below). If there are more then 2 columns, each column is considered a unique class, and a ROC graph and AUC score will be computed for each. A Macro-ROC and Micro-ROC are computed and plotted too by default. Based on scikit-learn examples (as was seen on April 2018): y_true : list / NumPy ndarray The true classes of the predicted data y_pred : list / NumPy ndarray The predicted classes micro : Boolean Default = True Whether to calculate a Micro ROC graph (not applicable for binary cases) macro : Boolean Default = True Whether to calculate a Macro ROC graph (not applicable for binary cases) eoptimal_threshold : Boolean Default = True Whether to calculate and display the estimated-optimal threshold for each ROC graph. The estimated-optimal threshold is the closest computed threshold with (fpr,tpr) values closest to (0,1) class_names : list or string Default = None Names of the different classes. In a multi-class classification, the order must match the order of the classes probabilities in the input data. In a binary classification, can be a string or a list. If a list, only the last element will be used. colors : list of Matplotlib color strings or None Default = None List of colors to be used for the plotted curves. If None , falls back to a predefined default. ax : matplotlib ax Default = None Matplotlib Axis on which the curves will be plotted figsize : (int,int) or None Default = None A Matplotlib figure-size tuple. If None , falls back to Matplotlib's default. Only used if ax=None . xlim : (float, float) Default = (0.,1.) X-axis limits. ylim : (float,float) Default = (0.,1.02) Y-axis limits. lw : int Default = 2 Line-width. ls : string Default = '-' Matplotlib line-style string ms : int Default = 10 Marker-size. fmt : string Default = '.2f' String formatting of displayed AUC and threshold numbers. Returns: A dictionary, one key for each class. Each value is another dictionary, holding AUC and eOpT values. Example: See examples . Binary Classification Input Example: Consider a data-set of two data-points where the true class of the first line is class 0, which was predicted with a probability of 0.6, and the second line's true class is 1, with predicted probability of 0.8. # First option: >> roc_graph ( y_true = [ 0 , 1 ], y_pred = [ 0.6 , 0.8 ]) # Second option: >> roc_graph ( y_true = [[ 1 , 0 ],[ 0 , 1 ]], y_pred = [[ 0.6 , 0.4 ],[ 0.2 , 0.8 ]]) # Both yield the same result","title":"roc_graph"},{"location":"modules/model_utils/#random_forest_feature_importance","text":"random_forest_feature_importance(forest, features, precision=4) Given a trained sklearn.ensemble.RandomForestClassifier , plot the different features based on their importance according to the classifier, from the most important to the least. forest : sklearn.ensemble.RandomForestClassifier A trained RandomForestClassifier features : list A list of the names of the features the classifier was trained on, ordered by the same order the appeared in the training data precision : int Default = 4 Precision of feature importance.","title":"random_forest_feature_importance"},{"location":"modules/nominal/","text":"nominal \u00b6 associations \u00b6 associations(dataset, nominal_columns='auto', mark_columns=False, theil_u=False, plot=True, clustering=False, bias_correction=True, nan_strategy=_REPLACE, nan_replace_value=_DEFAULT_REPLACE_VALUE, ax=None, figsize=None, annot=True, fmt='.2f', cmap=None, sv_color='silver') Calculate the correlation/strength-of-association of features in data-set with both categorical and continuous features using: * Pearson's R for continuous-continuous cases * Correlation Ratio for categorical-continuous cases * Cramer's V or Theil's U for categorical-categorical cases dataset : NumPy ndarray / Pandas DataFrame The data-set for which the features' correlation is computed nominal_columns : string / list / NumPy ndarray Names of columns of the data-set which hold categorical values. Can also be the string 'all' to state that all columns are categorical, 'auto' (default) to identify nominal columns automatically, or None to state none are categorical mark_columns : Boolean Default: False if True, output's columns' names will have a suffix of '(nom)' or '(con)' based on there type (eda_tools or continuous), as provided by nominal_columns theil_u : Boolean Default: False In the case of categorical-categorical feaures, use Theil's U instead of Cramer's V plot : Boolean Default: True Plot a heat-map of the correlation matrix clustering : Boolean Default: False If True, the computed associations will be sorted into groups by similar correlations bias_correction : Boolean Default = True Use bias correction for Cramer's V from Bergsma and Wicher, Journal of the Korean Statistical Society 42 (2013): 323-328. nan_strategy : string Default: 'replace' How to handle missing values: can be either 'drop_samples' to remove samples with missing values, 'drop_features' to remove features (columns) with missing values, or 'replace' to replace all missing values with the nan_replace_value. Missing values are None and np.nan. nan_replace_value : any Default: 0.0 The value used to replace missing values with. Only applicable when nan_strategy is set to 'replace' ax : matplotlib Axe Default = None Matplotlib Axis on which the heat-map will be plotted figsize : (int,int) or None Default = None A Matplotlib figure-size tuple. If None , falls back to Matplotlib's default. Only used if ax=None . annot : Boolean Default = True Plot number annotations on the heat-map fmt : string Default = '.2f' String formatting of annotations cmap : Matplotlib colormap or None Default = None A colormap to be used for the heat-map. If None, falls back to Seaborn's heat-map default sv_color : string Default = 'silver' A Matplotlib color. The color to be used when displaying single-value features over the heat-map cbar : Boolean Default = True Display heat-map's color-bar Returns: A dictionary with the following keys: corr : A DataFrame of the correlation/strength-of-association between all features ax : A Matplotlib Axe Example: See examples . cluster_correlations \u00b6 cluster_correlations(corr_mat, indexes=None) Apply agglomerative clustering in order to sort a correlation matrix. Based on this clustering example . corr_mat : Pandas DataFrame A correlation matrix (as output from associations ) indexes : list / NumPy ndarray / Pandas Series A sequence of cluster indexes for sorting. If not present, a clustering is performed. Returns: a sorted correlation matrix ( pd.DataFrame ) cluster indexes based on the original dataset ( list ) Example: >> assoc = associations ( customers , plot = False ) >> correlations = assoc [ 'corr' ] >> correlations , _ = cluster_correlations ( correlations ) compute_associations \u00b6 compute_associations(dataset, nominal_columns='auto', mark_columns=False, theil_u=False, plot=True, clustering=False, bias_correction=True, nan_strategy=_REPLACE, nan_replace_value=_DEFAULT_REPLACE_VALUE) Calculate the correlation/strength-of-association of features in data-set with both categorical and continuous features using: * Pearson's R for continuous-continuous cases * Correlation Ratio for categorical-continuous cases * Cramer's V or Theil's U for categorical-categorical cases This is equivalent to executing associations(data, plot=False, ...)['corr'] , only it skips entirely on the drawing phase of the heat-map (See issue #49 ). dataset : NumPy ndarray / Pandas DataFrame The data-set for which the features' correlation is computed nominal_columns : string / list / NumPy ndarray Names of columns of the data-set which hold categorical values. Can also be the string 'all' to state that all columns are categorical, 'auto' (default) to identify nominal columns automatically, or None to state none are categorical mark_columns : Boolean Default: False if True, output's columns' names will have a suffix of '(nom)' or '(con)' based on there type (eda_tools or continuous), as provided by nominal_columns theil_u : Boolean Default: False In the case of categorical-categorical feaures, use Theil's U instead of Cramer's V plot : Boolean Default: True Plot a heat-map of the correlation matrix clustering : Boolean Default: False If True, the computed associations will be sorted into groups by similar correlations bias_correction : Boolean Default = True Use bias correction for Cramer's V from Bergsma and Wicher, Journal of the Korean Statistical Society 42 (2013): 323-328. nan_strategy : string Default: 'replace' How to handle missing values: can be either 'drop_samples' to remove samples with missing values, 'drop_features' to remove features (columns) with missing values, or 'replace' to replace all missing values with the nan_replace_value. Missing values are None and np.nan. nan_replace_value : any Default: 0.0 The value used to replace missing values with. Only applicable when nan_strategy is set to 'replace' Returns: A DataFrame of the correlation/strength-of-association between all features conditional_entropy \u00b6 conditional_entropy(x, y, nan_strategy=REPLACE, nan_replace_value=DEFAULT_REPLACE_VALUE, log_base=math.e) Given measurements x and y of random variables X X and Y Y , calculates the conditional entropy of X X given Y Y : S(X|Y) = - \\sum_{x,y} p(x,y) \\log\\frac{p(x,y)}{p(y)} S(X|Y) = - \\sum_{x,y} p(x,y) \\log\\frac{p(x,y)}{p(y)} Read more on Wikipedia . x : list / NumPy ndarray / Pandas Series A sequence of measurements y : list / NumPy ndarray / Pandas Series A sequence of measurements nan_strategy : string Default: 'replace' How to handle missing values: can be either 'drop' to remove samples with missing values, or 'replace' to replace all missing values with the nan_replace_value. Missing values are None and np.nan. nan_replace_value : any Default: 0.0 The value used to replace missing values with. Only applicable when nan_strategy is set to 'replace'. log_base : float Default: math.e Specifying base for calculating entropy. Returns: float correlation_ratio \u00b6 correlation_ratio(categories, measurements, nan_strategy=REPLACE, nan_replace_value=DEFAULT_REPLACE_VALUE) Calculates the Correlation Ratio ( \\eta \\eta ) for categorical-continuous association: \\eta = \\sqrt{\\frac{\\sum_x{n_x (\\bar{y}_x - \\bar{y})^2}}{\\sum_{x,i}{(y_{xi}-\\bar{y})^2}}} \\eta = \\sqrt{\\frac{\\sum_x{n_x (\\bar{y}_x - \\bar{y})^2}}{\\sum_{x,i}{(y_{xi}-\\bar{y})^2}}} where n_x n_x is the number of observations in category x x , and we define: \\bar{y}_x = \\frac{\\sum_i{y_{xi}}}{n_x} , \\bar{y} = \\frac{\\sum_i{n_x \\bar{y}_x}}{\\sum_x{n_x}} \\bar{y}_x = \\frac{\\sum_i{y_{xi}}}{n_x} , \\bar{y} = \\frac{\\sum_i{n_x \\bar{y}_x}}{\\sum_x{n_x}} Answers the question - given a continuous value of a measurement, is it possible to know which category is it associated with? Value is in the range [0,1], where 0 means a category cannot be determined by a continuous measurement, and 1 means a category can be determined with absolute certainty. Read more on Wikipedia . categories : list / NumPy ndarray / Pandas Series A sequence of categorical measurements measurements : list / NumPy ndarray / Pandas Series A sequence of continuous measurements nan_strategy : string Default: 'replace' How to handle missing values: can be either 'drop' to remove samples with missing values, or 'replace' to replace all missing values with the nan_replace_value. Missing values are None and np.nan. nan_replace_value : any Default: 0.0 The value used to replace missing values with. Only applicable when nan_strategy is set to 'replace'. Returns: float in the range of [0,1] cramers_v \u00b6 cramers_v(x, y, bias_correction=True, nan_strategy=REPLACE, nan_replace_value=DEFAULT_REPLACE_VALUE) Calculates Cramer's V statistic for categorical-categorical association. This is a symmetric coefficient: V(x,y) = V(y,x) V(x,y) = V(y,x) . Read more on Wikipedia . Original function taken from this answer on StackOverflow. x : list / NumPy ndarray / Pandas Series A sequence of categorical measurements y : list / NumPy ndarray / Pandas Series A sequence of categorical measurements bias_correction : Boolean Default = True Use bias correction from Bergsma and Wicher, Journal of the Korean Statistical Society 42 (2013): 323-328. nan_strategy : string Default: 'replace' How to handle missing values: can be either 'drop' to remove samples with missing values, or 'replace' to replace all missing values with the nan_replace_value. Missing values are None and np.nan. nan_replace_value : any Default: 0.0 The value used to replace missing values with. Only applicable when nan_strategy is set to 'replace'. Returns: float in the range of [0,1] identify_nominal_columns \u00b6 identify_nominal_columns(dataset, include=['object', 'category']) Given a dataset, identify categorical columns. This is used internally in associations and numerical_encoding , but can also be used directly. dataset : pd.DataFrame include : list Default: ['object', 'category'] which column types to filter by. Returns: list of categorical columns Example: >> df = pd . DataFrame ({ 'col1' : [ 'a' , 'b' , 'c' , 'a' ], 'col2' : [ 3 , 4 , 2 , 1 ]}) >> identify_nominal_columns ( df ) [ 'col1' ] numerical_encoding \u00b6 numerical_encoding(dataset, nominal_columns='auto', drop_single_label=False, drop_fact_dict=True, nan_strategy=REPLACE, nan_replace_value=DEFAULT_REPLACE_VALUE) Encoding a data-set with mixed data (numerical and categorical) to a numerical-only data-set, using the following logic: categorical with only a single value will be marked as zero (or dropped, if requested) categorical with two values will be replaced with the result of Pandas factorize categorical with more than two values will be replaced with the result of Pandas get_dummies numerical columns will not be modified dataset : NumPy ndarray / Pandas DataFrame The data-set to encode nominal_columns : sequence / string Default: 'auto' Names of columns of the data-set which hold categorical values. Can also be the string 'all' to state that all columns are categorical, 'auto' (default) to identify nominal columns automatically, or None to state none are categorical (nothing happens) drop_single_label : Boolean Default: False If True, nominal columns with a only a single value will be dropped. drop_fact_dict : Boolean Default: True If True, the return value will be the encoded DataFrame alone. If False, it will be a tuple of the DataFrame and the dictionary of the binary factorization (originating from pd.factorize) nan_strategy : string Default: 'replace' How to handle missing values: can be either 'drop_samples' to remove samples with missing values, 'drop_features' to remove features (columns) with missing values, or 'replace' to replace all missing values with the nan_replace_value. Missing values are None and np.nan. nan_replace_value : any Default: 0.0 The value used to replace missing values with. Only applicable when nan_strategy is set to 'replace' Returns: pd.DataFrame or (pd.DataFrame, dict) . If drop_fact_dict is True, returns the encoded DataFrame. else, returns a tuple of the encoded DataFrame and dictionary, where each key is a two-value column, and the value is the original labels, as supplied by Pandas factorize . Will be empty if no two-value columns are present in the data-set theils_u \u00b6 theils_u(x, y, nan_strategy=REPLACE, nan_replace_value=DEFAULT_REPLACE_VALUE) Calculates Theil's U statistic (Uncertainty coefficient) for categorical-categorical association, defined as: U(X|Y) = \\frac{S(X) - S(X|Y)}{S(X)} U(X|Y) = \\frac{S(X) - S(X|Y)}{S(X)} where S(X) S(X) is the entropy of X X and S(X|Y) S(X|Y) is the conditional entropy of X X given Y Y . This is the uncertainty of x given y: value is on the range of [0,1] - where 0 means y provides no information about x, and 1 means y provides full information about x. This is an asymmetric coefficient: U(x,y) \\neq U(y,x) U(x,y) \\neq U(y,x) . Read more on Wikipedia . x : list / NumPy ndarray / Pandas Series A sequence of categorical measurements y : list / NumPy ndarray / Pandas Series A sequence of categorical measurements nan_strategy : string Default: 'replace' How to handle missing values: can be either 'drop' to remove samples with missing values, or 'replace' to replace all missing values with the nan_replace_value. Missing values are None and np.nan. nan_replace_value : any Default: 0.0 The value used to replace missing values with. Only applicable when nan_strategy is set to 'replace'. Returns: float in the range of [0,1]","title":"nominal"},{"location":"modules/nominal/#nominal","text":"","title":"nominal"},{"location":"modules/nominal/#associations","text":"associations(dataset, nominal_columns='auto', mark_columns=False, theil_u=False, plot=True, clustering=False, bias_correction=True, nan_strategy=_REPLACE, nan_replace_value=_DEFAULT_REPLACE_VALUE, ax=None, figsize=None, annot=True, fmt='.2f', cmap=None, sv_color='silver') Calculate the correlation/strength-of-association of features in data-set with both categorical and continuous features using: * Pearson's R for continuous-continuous cases * Correlation Ratio for categorical-continuous cases * Cramer's V or Theil's U for categorical-categorical cases dataset : NumPy ndarray / Pandas DataFrame The data-set for which the features' correlation is computed nominal_columns : string / list / NumPy ndarray Names of columns of the data-set which hold categorical values. Can also be the string 'all' to state that all columns are categorical, 'auto' (default) to identify nominal columns automatically, or None to state none are categorical mark_columns : Boolean Default: False if True, output's columns' names will have a suffix of '(nom)' or '(con)' based on there type (eda_tools or continuous), as provided by nominal_columns theil_u : Boolean Default: False In the case of categorical-categorical feaures, use Theil's U instead of Cramer's V plot : Boolean Default: True Plot a heat-map of the correlation matrix clustering : Boolean Default: False If True, the computed associations will be sorted into groups by similar correlations bias_correction : Boolean Default = True Use bias correction for Cramer's V from Bergsma and Wicher, Journal of the Korean Statistical Society 42 (2013): 323-328. nan_strategy : string Default: 'replace' How to handle missing values: can be either 'drop_samples' to remove samples with missing values, 'drop_features' to remove features (columns) with missing values, or 'replace' to replace all missing values with the nan_replace_value. Missing values are None and np.nan. nan_replace_value : any Default: 0.0 The value used to replace missing values with. Only applicable when nan_strategy is set to 'replace' ax : matplotlib Axe Default = None Matplotlib Axis on which the heat-map will be plotted figsize : (int,int) or None Default = None A Matplotlib figure-size tuple. If None , falls back to Matplotlib's default. Only used if ax=None . annot : Boolean Default = True Plot number annotations on the heat-map fmt : string Default = '.2f' String formatting of annotations cmap : Matplotlib colormap or None Default = None A colormap to be used for the heat-map. If None, falls back to Seaborn's heat-map default sv_color : string Default = 'silver' A Matplotlib color. The color to be used when displaying single-value features over the heat-map cbar : Boolean Default = True Display heat-map's color-bar Returns: A dictionary with the following keys: corr : A DataFrame of the correlation/strength-of-association between all features ax : A Matplotlib Axe Example: See examples .","title":"associations"},{"location":"modules/nominal/#cluster_correlations","text":"cluster_correlations(corr_mat, indexes=None) Apply agglomerative clustering in order to sort a correlation matrix. Based on this clustering example . corr_mat : Pandas DataFrame A correlation matrix (as output from associations ) indexes : list / NumPy ndarray / Pandas Series A sequence of cluster indexes for sorting. If not present, a clustering is performed. Returns: a sorted correlation matrix ( pd.DataFrame ) cluster indexes based on the original dataset ( list ) Example: >> assoc = associations ( customers , plot = False ) >> correlations = assoc [ 'corr' ] >> correlations , _ = cluster_correlations ( correlations )","title":"cluster_correlations"},{"location":"modules/nominal/#compute_associations","text":"compute_associations(dataset, nominal_columns='auto', mark_columns=False, theil_u=False, plot=True, clustering=False, bias_correction=True, nan_strategy=_REPLACE, nan_replace_value=_DEFAULT_REPLACE_VALUE) Calculate the correlation/strength-of-association of features in data-set with both categorical and continuous features using: * Pearson's R for continuous-continuous cases * Correlation Ratio for categorical-continuous cases * Cramer's V or Theil's U for categorical-categorical cases This is equivalent to executing associations(data, plot=False, ...)['corr'] , only it skips entirely on the drawing phase of the heat-map (See issue #49 ). dataset : NumPy ndarray / Pandas DataFrame The data-set for which the features' correlation is computed nominal_columns : string / list / NumPy ndarray Names of columns of the data-set which hold categorical values. Can also be the string 'all' to state that all columns are categorical, 'auto' (default) to identify nominal columns automatically, or None to state none are categorical mark_columns : Boolean Default: False if True, output's columns' names will have a suffix of '(nom)' or '(con)' based on there type (eda_tools or continuous), as provided by nominal_columns theil_u : Boolean Default: False In the case of categorical-categorical feaures, use Theil's U instead of Cramer's V plot : Boolean Default: True Plot a heat-map of the correlation matrix clustering : Boolean Default: False If True, the computed associations will be sorted into groups by similar correlations bias_correction : Boolean Default = True Use bias correction for Cramer's V from Bergsma and Wicher, Journal of the Korean Statistical Society 42 (2013): 323-328. nan_strategy : string Default: 'replace' How to handle missing values: can be either 'drop_samples' to remove samples with missing values, 'drop_features' to remove features (columns) with missing values, or 'replace' to replace all missing values with the nan_replace_value. Missing values are None and np.nan. nan_replace_value : any Default: 0.0 The value used to replace missing values with. Only applicable when nan_strategy is set to 'replace' Returns: A DataFrame of the correlation/strength-of-association between all features","title":"compute_associations"},{"location":"modules/nominal/#conditional_entropy","text":"conditional_entropy(x, y, nan_strategy=REPLACE, nan_replace_value=DEFAULT_REPLACE_VALUE, log_base=math.e) Given measurements x and y of random variables X X and Y Y , calculates the conditional entropy of X X given Y Y : S(X|Y) = - \\sum_{x,y} p(x,y) \\log\\frac{p(x,y)}{p(y)} S(X|Y) = - \\sum_{x,y} p(x,y) \\log\\frac{p(x,y)}{p(y)} Read more on Wikipedia . x : list / NumPy ndarray / Pandas Series A sequence of measurements y : list / NumPy ndarray / Pandas Series A sequence of measurements nan_strategy : string Default: 'replace' How to handle missing values: can be either 'drop' to remove samples with missing values, or 'replace' to replace all missing values with the nan_replace_value. Missing values are None and np.nan. nan_replace_value : any Default: 0.0 The value used to replace missing values with. Only applicable when nan_strategy is set to 'replace'. log_base : float Default: math.e Specifying base for calculating entropy. Returns: float","title":"conditional_entropy"},{"location":"modules/nominal/#correlation_ratio","text":"correlation_ratio(categories, measurements, nan_strategy=REPLACE, nan_replace_value=DEFAULT_REPLACE_VALUE) Calculates the Correlation Ratio ( \\eta \\eta ) for categorical-continuous association: \\eta = \\sqrt{\\frac{\\sum_x{n_x (\\bar{y}_x - \\bar{y})^2}}{\\sum_{x,i}{(y_{xi}-\\bar{y})^2}}} \\eta = \\sqrt{\\frac{\\sum_x{n_x (\\bar{y}_x - \\bar{y})^2}}{\\sum_{x,i}{(y_{xi}-\\bar{y})^2}}} where n_x n_x is the number of observations in category x x , and we define: \\bar{y}_x = \\frac{\\sum_i{y_{xi}}}{n_x} , \\bar{y} = \\frac{\\sum_i{n_x \\bar{y}_x}}{\\sum_x{n_x}} \\bar{y}_x = \\frac{\\sum_i{y_{xi}}}{n_x} , \\bar{y} = \\frac{\\sum_i{n_x \\bar{y}_x}}{\\sum_x{n_x}} Answers the question - given a continuous value of a measurement, is it possible to know which category is it associated with? Value is in the range [0,1], where 0 means a category cannot be determined by a continuous measurement, and 1 means a category can be determined with absolute certainty. Read more on Wikipedia . categories : list / NumPy ndarray / Pandas Series A sequence of categorical measurements measurements : list / NumPy ndarray / Pandas Series A sequence of continuous measurements nan_strategy : string Default: 'replace' How to handle missing values: can be either 'drop' to remove samples with missing values, or 'replace' to replace all missing values with the nan_replace_value. Missing values are None and np.nan. nan_replace_value : any Default: 0.0 The value used to replace missing values with. Only applicable when nan_strategy is set to 'replace'. Returns: float in the range of [0,1]","title":"correlation_ratio"},{"location":"modules/nominal/#cramers_v","text":"cramers_v(x, y, bias_correction=True, nan_strategy=REPLACE, nan_replace_value=DEFAULT_REPLACE_VALUE) Calculates Cramer's V statistic for categorical-categorical association. This is a symmetric coefficient: V(x,y) = V(y,x) V(x,y) = V(y,x) . Read more on Wikipedia . Original function taken from this answer on StackOverflow. x : list / NumPy ndarray / Pandas Series A sequence of categorical measurements y : list / NumPy ndarray / Pandas Series A sequence of categorical measurements bias_correction : Boolean Default = True Use bias correction from Bergsma and Wicher, Journal of the Korean Statistical Society 42 (2013): 323-328. nan_strategy : string Default: 'replace' How to handle missing values: can be either 'drop' to remove samples with missing values, or 'replace' to replace all missing values with the nan_replace_value. Missing values are None and np.nan. nan_replace_value : any Default: 0.0 The value used to replace missing values with. Only applicable when nan_strategy is set to 'replace'. Returns: float in the range of [0,1]","title":"cramers_v"},{"location":"modules/nominal/#identify_nominal_columns","text":"identify_nominal_columns(dataset, include=['object', 'category']) Given a dataset, identify categorical columns. This is used internally in associations and numerical_encoding , but can also be used directly. dataset : pd.DataFrame include : list Default: ['object', 'category'] which column types to filter by. Returns: list of categorical columns Example: >> df = pd . DataFrame ({ 'col1' : [ 'a' , 'b' , 'c' , 'a' ], 'col2' : [ 3 , 4 , 2 , 1 ]}) >> identify_nominal_columns ( df ) [ 'col1' ]","title":"identify_nominal_columns"},{"location":"modules/nominal/#numerical_encoding","text":"numerical_encoding(dataset, nominal_columns='auto', drop_single_label=False, drop_fact_dict=True, nan_strategy=REPLACE, nan_replace_value=DEFAULT_REPLACE_VALUE) Encoding a data-set with mixed data (numerical and categorical) to a numerical-only data-set, using the following logic: categorical with only a single value will be marked as zero (or dropped, if requested) categorical with two values will be replaced with the result of Pandas factorize categorical with more than two values will be replaced with the result of Pandas get_dummies numerical columns will not be modified dataset : NumPy ndarray / Pandas DataFrame The data-set to encode nominal_columns : sequence / string Default: 'auto' Names of columns of the data-set which hold categorical values. Can also be the string 'all' to state that all columns are categorical, 'auto' (default) to identify nominal columns automatically, or None to state none are categorical (nothing happens) drop_single_label : Boolean Default: False If True, nominal columns with a only a single value will be dropped. drop_fact_dict : Boolean Default: True If True, the return value will be the encoded DataFrame alone. If False, it will be a tuple of the DataFrame and the dictionary of the binary factorization (originating from pd.factorize) nan_strategy : string Default: 'replace' How to handle missing values: can be either 'drop_samples' to remove samples with missing values, 'drop_features' to remove features (columns) with missing values, or 'replace' to replace all missing values with the nan_replace_value. Missing values are None and np.nan. nan_replace_value : any Default: 0.0 The value used to replace missing values with. Only applicable when nan_strategy is set to 'replace' Returns: pd.DataFrame or (pd.DataFrame, dict) . If drop_fact_dict is True, returns the encoded DataFrame. else, returns a tuple of the encoded DataFrame and dictionary, where each key is a two-value column, and the value is the original labels, as supplied by Pandas factorize . Will be empty if no two-value columns are present in the data-set","title":"numerical_encoding"},{"location":"modules/nominal/#theils_u","text":"theils_u(x, y, nan_strategy=REPLACE, nan_replace_value=DEFAULT_REPLACE_VALUE) Calculates Theil's U statistic (Uncertainty coefficient) for categorical-categorical association, defined as: U(X|Y) = \\frac{S(X) - S(X|Y)}{S(X)} U(X|Y) = \\frac{S(X) - S(X|Y)}{S(X)} where S(X) S(X) is the entropy of X X and S(X|Y) S(X|Y) is the conditional entropy of X X given Y Y . This is the uncertainty of x given y: value is on the range of [0,1] - where 0 means y provides no information about x, and 1 means y provides full information about x. This is an asymmetric coefficient: U(x,y) \\neq U(y,x) U(x,y) \\neq U(y,x) . Read more on Wikipedia . x : list / NumPy ndarray / Pandas Series A sequence of categorical measurements y : list / NumPy ndarray / Pandas Series A sequence of categorical measurements nan_strategy : string Default: 'replace' How to handle missing values: can be either 'drop' to remove samples with missing values, or 'replace' to replace all missing values with the nan_replace_value. Missing values are None and np.nan. nan_replace_value : any Default: 0.0 The value used to replace missing values with. Only applicable when nan_strategy is set to 'replace'. Returns: float in the range of [0,1]","title":"theils_u"},{"location":"modules/sampling/","text":"sampling \u00b6 boltzmann_sampling \u00b6 boltzmann_sampling(numbers, k=1, with_replacement=False) Return k numbers from a boltzmann-sampling over the supplied numbers numbers : List or np.ndarray numbers to sample k : int Default: 1 How many numbers to sample. Choosing k=None will yield a single number with_replacement : Boolean Default: False Allow replacement or not Returns: list , np.ndarray or a single number (depending on the input) weighted_sampling \u00b6 weighted_sampling(numbers, k=1, with_replacement=False) Return k numbers from a weighted-sampling over the supplied numbers numbers : List or np.ndarray numbers to sample k : int Default: 1 How many numbers to sample. Choosing k=None will yield a single number with_replacement : Boolean Default: False Allow replacement or not Returns: list , np.ndarray or a single number (depending on the input)","title":"sampling"},{"location":"modules/sampling/#sampling","text":"","title":"sampling"},{"location":"modules/sampling/#boltzmann_sampling","text":"boltzmann_sampling(numbers, k=1, with_replacement=False) Return k numbers from a boltzmann-sampling over the supplied numbers numbers : List or np.ndarray numbers to sample k : int Default: 1 How many numbers to sample. Choosing k=None will yield a single number with_replacement : Boolean Default: False Allow replacement or not Returns: list , np.ndarray or a single number (depending on the input)","title":"boltzmann_sampling"},{"location":"modules/sampling/#weighted_sampling","text":"weighted_sampling(numbers, k=1, with_replacement=False) Return k numbers from a weighted-sampling over the supplied numbers numbers : List or np.ndarray numbers to sample k : int Default: 1 How many numbers to sample. Choosing k=None will yield a single number with_replacement : Boolean Default: False Allow replacement or not Returns: list , np.ndarray or a single number (depending on the input)","title":"weighted_sampling"}]}